{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”¤ ExtendedTokenizer - Train 300K BPE Tokenizer\n",
        "\n",
        "This notebook trains a 300,000 token BPE tokenizer on Simple English Wikipedia.\n",
        "\n",
        "**Estimated time:** ~1-2 hours on free Colab\n",
        "\n",
        "**Requirements:** None (free Colab works fine)"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies"
      ],
      "metadata": {
        "id": "install-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/marcelo-earth/extended-tokenizer.git\n",
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Download Simple English Wikipedia"
      ],
      "metadata": {
        "id": "download-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Simple English Wikipedia dump URL\n",
        "DUMP_URL = \"https://dumps.wikimedia.org/simplewiki/latest/simplewiki-latest-pages-articles.xml.bz2\"\n",
        "OUTPUT_DIR = \"data\"\n",
        "DUMP_PATH = f\"{OUTPUT_DIR}/simplewiki-latest-pages-articles.xml.bz2\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "if os.path.exists(DUMP_PATH):\n",
        "    print(f\"File already exists: {DUMP_PATH}\")\n",
        "else:\n",
        "    print(f\"Downloading Simple English Wikipedia...\")\n",
        "    print(f\"URL: {DUMP_URL}\")\n",
        "    \n",
        "    response = requests.get(DUMP_URL, stream=True)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    total_size = int(response.headers.get(\"content-length\", 0))\n",
        "    \n",
        "    with open(DUMP_PATH, \"wb\") as f:\n",
        "        with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Downloading\") as pbar:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "                pbar.update(len(chunk))\n",
        "    \n",
        "    print(f\"\\nDownloaded to {DUMP_PATH}\")\n",
        "\n",
        "# Show file size\n",
        "size_mb = os.path.getsize(DUMP_PATH) / (1024 * 1024)\n",
        "print(f\"File size: {size_mb:.1f} MB\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train the Tokenizer\n",
        "\n",
        "This will:\n",
        "1. Extract and preprocess articles from the Wikipedia dump\n",
        "2. Train BPE with 300,000 vocabulary size\n",
        "3. Save the vocabulary files"
      ],
      "metadata": {
        "id": "train-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from extended_tokenizer.trainer import BPETrainer\n",
        "from extended_tokenizer.data.wikipedia import stream_wikipedia_texts\n",
        "\n",
        "# Configuration\n",
        "VOCAB_SIZE = 300_000\n",
        "MIN_FREQUENCY = 2\n",
        "OUTPUT_PATH = \"vocab/bpe_300k\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"BPE Tokenizer Training\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nVocab size: {VOCAB_SIZE:,}\")\n",
        "print(f\"Min frequency: {MIN_FREQUENCY}\")\n",
        "print(f\"Output: {OUTPUT_PATH}\")\n",
        "print()\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = BPETrainer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    min_frequency=MIN_FREQUENCY,\n",
        ")\n",
        "\n",
        "# Stream articles from Wikipedia\n",
        "print(\"Loading Wikipedia articles...\")\n",
        "corpus = stream_wikipedia_texts(\n",
        "    DUMP_PATH,\n",
        "    max_articles=None,  # Use all articles\n",
        "    min_length=200,\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"\\nStarting BPE training...\")\n",
        "print(\"This may take 1-2 hours.\\n\")\n",
        "\n",
        "vocab = trainer.train(corpus, show_progress=True)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"Training Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Final vocabulary size: {vocab.vocab_size:,}\")\n",
        "print(f\"Number of merges: {vocab.num_merges:,}\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Save Vocabulary"
      ],
      "metadata": {
        "id": "save-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "trainer.save(OUTPUT_PATH)\n",
        "\n",
        "print(f\"Vocabulary saved to {OUTPUT_PATH}/\")\n",
        "print()\n",
        "print(\"Files created:\")\n",
        "for f in os.listdir(OUTPUT_PATH):\n",
        "    size = os.path.getsize(f\"{OUTPUT_PATH}/{f}\") / (1024 * 1024)\n",
        "    print(f\"  - {f}: {size:.2f} MB\")"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Test the Tokenizer"
      ],
      "metadata": {
        "id": "test-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from extended_tokenizer import ExtendedTokenizer\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = ExtendedTokenizer(vocab_path=OUTPUT_PATH)\n",
        "\n",
        "print(f\"Loaded tokenizer with {tokenizer.vocab_size:,} tokens\")\n",
        "print()\n",
        "\n",
        "# Test encoding/decoding\n",
        "test_texts = [\n",
        "    \"Hello, world!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is a programming language.\",\n",
        "    \"æ—¥æœ¬èªžãƒ†ã‚¹ãƒˆ\",  # Japanese\n",
        "    \"ðŸŽ‰ Emoji test! ðŸš€\",\n",
        "]\n",
        "\n",
        "print(\"Encoding tests:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = tokenizer.encode(text)\n",
        "    decoded = tokenizer.decode(tokens)\n",
        "    status = \"âœ“\" if decoded == text else \"âœ—\"\n",
        "    print(f\"{status} '{text}'\")\n",
        "    print(f\"   Tokens: {len(tokens)} -> {tokens[:10]}{'...' if len(tokens) > 10 else ''}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Upload to HuggingFace Hub (Optional)\n",
        "\n",
        "Run this cell to upload the trained vocabulary to HuggingFace Hub."
      ],
      "metadata": {
        "id": "upload-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, login to HuggingFace\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# This will prompt for your HuggingFace token\n",
        "login()"
      ],
      "metadata": {
        "id": "hf-login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Configuration - change this to your repo\n",
        "REPO_ID = \"marcelo-earth/extended-tokenizer-300k\"\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Create repo if it doesn't exist\n",
        "try:\n",
        "    api.create_repo(repo_id=REPO_ID, exist_ok=True)\n",
        "    print(f\"Repository ready: https://huggingface.co/{REPO_ID}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: {e}\")\n",
        "\n",
        "# Upload files\n",
        "print(\"\\nUploading files...\")\n",
        "api.upload_folder(\n",
        "    folder_path=OUTPUT_PATH,\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Upload complete!\")\n",
        "print(f\"View at: https://huggingface.co/{REPO_ID}\")"
      ],
      "metadata": {
        "id": "upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Download Vocab Files (Alternative)\n",
        "\n",
        "If you don't want to upload to HuggingFace, you can download the files directly."
      ],
      "metadata": {
        "id": "download-files-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create a zip file\n",
        "shutil.make_archive(\"extended_tokenizer_300k\", \"zip\", OUTPUT_PATH)\n",
        "\n",
        "# Download\n",
        "files.download(\"extended_tokenizer_300k.zip\")\n",
        "print(\"\\nDownload started!\")"
      ],
      "metadata": {
        "id": "download-files"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
